
# Dependencies:
import cv2
import numpy as np
import pyvista as pv
from dataclasses import dataclass
from pathlib import Path
from PIL import Image
from matplotlib import pyplot as plt
from sklearn.cluster._hdbscan import hdbscan
from sklearn.preprocessing import StandardScaler
from tifffile import tifffile

# TODO: Break down the StackedImage class into multiple smaller classes for clarity (future release)

# TODO: 1) update remove_vignette as described (current release)

class StackedImage:
    """
    Processes stacked TIFF images generated by FrameStack.

    This class provides functionality for processing stacked images,
    including downsampling, segmentation, vignette removal, background
    neutralization, and dynamic range adjustment.
    """

    @dataclass(frozen=True)
    class StackedImageInputs:
        data_directory: Path
        verbosity: int = 1

    def __init__(self, **kwargs):

        self.inputs = self.StackedImageInputs(**kwargs)
        self.rgb_raw = self._load_from_tiff().astype(np.float64)
        self.rgb_processing = self.rgb_raw.copy().astype(np.float64)
        self.background_mask = None

        # Initialise processing metadata.
        self.processing_details = {
            "dynamic_range_boosted": 0,
            "background_neutralised": 0,
            "noise_reduced": 0,
            "vignette_corrected": 0,
        }

    def correct_white_balance(self):
        """
        Normalises image white balance by anchoring RGB channels to the background Blue reference.

        Calculates per-channel scaling factors based on the background's
        distance from pure white (255). It then applies these factors to the inverted
        color space to align the Red and Green background intensities with the Blue channel.

        Returns:
            self: The instance with updated 'rgb_processing' and corrected white balance.
        """

        if self.background_mask is None:
            self._extract_background_with_segmentation()

        print("Correcting white balance...")

        # Note that background mask is a 3D array
        rgb_background = (self.rgb_processing * self.background_mask)

        # Must be 64 bit to maintain precision when computing large sums, like mean average
        rgb_background.astype(np.float64)

        rgb_means = np.nanmean(np.where(rgb_background > 0, rgb_background, np.nan), axis=(0, 1))
        k_colour_correct = (255 - rgb_means[2]) / (255 - rgb_means)

        self.rgb_processing = 255.0 - k_colour_correct * (255.0 - self.rgb_processing)
        self.rgb_processing = np.clip(self.rgb_processing, 0, 255).astype(np.float64)

        print("... process completed")

        if self.inputs.verbosity > 0:
            plt.figure(figsize=(16, 12))
            plt.imshow(self.rgb_processing / 255.0)
            plt.show()

        return self

    def remove_vignette(self, strength=1.0, degree=2):
        """Corrects radial illumination falloff using a masked polynomial fit.

        Models the background curvature as a function of squared radius (r^2)
        using Ordinary Least Squares. The resulting profile is used to
        flatten the intensity across all image channels.

        Args:
            strength (float): Intensity of the correction; 1.0 is full application.
            degree (int): Radial polynomial degree (even integers, e.g., 2, 4).

        Returns:
            self: The class instance with the corrected image in `self.rgb_processing`.
        """

        if self.background_mask is None:
            self._extract_background_with_segmentation()

        print("Removing vignette...")

        rgb = self.rgb_processing.copy().astype(np.float64)
        rgb_curvature_corrected = np.empty_like(rgb)
        height, width, n_channels = rgb.shape

        y_centre = (height - 1) / 2.0
        x_centre = (width - 1) / 2.0

        y_grid, x_grid = np.mgrid[0:height, 0:width]
        r2_grid = (x_grid - x_centre) ** 2 + (y_grid - y_centre) ** 2

        # The background mask is in 3d, so it needs to be flattened to 2d. A check is added for robustness
        if self.background_mask.ndim == 3:
            mask_2d = np.any(self.background_mask, axis=-1)
        else:
            mask_2d = self.background_mask

        # Re-order mask to be a 1d array
        mask_1d = mask_2d.flatten() > 0

        # Successively add additional terms based on power of radial fit model
        powers = range(0, (degree // 2) + 1)
        features = []
        for p in powers:
            features.append(r2_grid ** p)

        # The design matrix is fed into the least squares solver
        design_matrix = np.stack(features, axis=-1).reshape(-1, len(features))
        design_matrix_masked = design_matrix[mask_1d]

        # Fit and apply curvature corrections for each channel
        for i in range(n_channels):

            single_channel_1d = rgb[:, :, i].reshape(-1)
            single_channel_1d_masked = single_channel_1d[mask_1d]

            # Solve least squares to find coefficients mapping each co-ordinate from background curvature to rgb data.
            # These fitting coefficients match the form of the equation z = a + b*r^2 + c*r^4 ...
            # They have the following relationship:
            # design_matrix_masked * fitting_coefficients = single_channel_1d_masked
            fitting_coefficients, *_ = np.linalg.lstsq(design_matrix_masked, single_channel_1d_masked, rcond=None)

            # Calculate the background model for the entire image by performing a dot product (@)
            curvature_correction_1d = design_matrix @ fitting_coefficients
            curvature_correction = curvature_correction_1d.reshape(height, width)

            # Create a 2D boolean mask from the first channel if it's 3D
            if self.background_mask.ndim == 3:
                mask_2d = self.background_mask[:, :, 0] > 0
            else:
                mask_2d = self.background_mask > 0

            # Apply the mean normalisation using the 2D mask. Calculate the mean only from the pixels designated as background
            # TODO: Is there a better way to compute the offsets? (future release)
            mean_curvature = np.mean(curvature_correction[mask_2d])
            curvature_offset = curvature_correction - mean_curvature

            # Subtract the fitted curvature from the original channel
            rgb_curvature_corrected[:, :, i] = rgb[:, :, i] - (strength * curvature_offset)

        # Clip data, store as 64 bit, save in Self
        rgb_curvature_corrected = np.clip(rgb_curvature_corrected, 0, 255)
        self.rgb_processing = rgb_curvature_corrected.astype(np.float64)

        print("... process completed")

        if self.inputs.verbosity > 0:
            fig, ax = plt.subplots(1, 2, figsize=(20, 10))
            ax[0].imshow(self.rgb_processing.astype(np.uint8))
            ax[0].set_title("Original")
            ax[1].imshow(rgb_curvature_corrected)
            ax[1].set_title("Vignette Removed (Masked Fit)")
            plt.show()

        return self

    def boost_dynamic_range(self, gamma: float=4):

        # TODO: Make this function more comprehensive, by adding Uses Arcsinh-like scaling and local contrast enhancement (future release)

        self.rgb_processing = self._adjust_gamma(self.rgb_processing,gamma=gamma)

        # Store results
        if self.inputs.verbosity > 0:
            plt.figure(figsize=(16, 12))
            plt.imshow(self.rgb_processing.astype(np.uint32))
            plt.show()

        return self

    def save_image_as_tiff(self):
        """Saves processed RGB data to TIFF. Data is converted from float32, used for processing, to 16bit uint8, for TIFF format"""

        save_data = np.round(np.clip(self.rgb_processing.astype(np.float32), 0, 255) * 257).astype(np.uint16)
        save_path = self.inputs.data_directory / "outputs" / "processed_rgb.tiff"
        tifffile.imwrite(save_path,save_data)

        return self

    def _load_from_tiff(self):
        """Loads the stacked TIFF as a float32 NumPy array"""

        target_file_path = self.inputs.data_directory / "outputs" / "processing_stacked_frame_rgb.tiff"
        if not target_file_path.exists():
            raise FileNotFoundError(f"Stacked image not found: {target_file_path}")
        print(f"Loading stacked image: {target_file_path}")

        with Image.open(target_file_path) as img:
            return np.array(img).astype(np.float64)

    def _extract_background_with_segmentation(self):
        """
        Segments the image using HDBSCAN on color and spatial features to isolate the background.

        This method downsamples the raw RGB data, applies gamma correction, and clusters
        pixels based on normalized 5D features (R, G, B, X, Y). It identifies the
        background by finding the most frequent cluster label present along the image
        edges. The final background mask is stored in `self.background_mask`.

        Returns:
            self: The instance with self.background_mask updated.
                  (1 = background pixel, 0 = non-background pixel).
        """

        # These settings change how the background segmentation process runs, and for now they will be kept as default values
        # TODO: Add the ability to tweak the background segmentation settings (future release)
        downsample_factor = 10
        gamma = 6
        cluster_sensitivity = 5E-4

        print("Extracting background with segmentation...")

        # Image is downsampled to reduce computational effort
        rgb = self._downsample_with_binning(image=self.rgb_processing, downsample_factor=downsample_factor)
        rgb = self._adjust_gamma(image=rgb, gamma=gamma)
        h, w, c = rgb.shape
        rgb_pixels = rgb.reshape(-1, c)

        y_coords, x_coords = np.indices((h,w))
        xy_coords_grid = np.stack(arrays=(x_coords, y_coords), axis=-1)
        xy_coords_1d_array = xy_coords_grid.reshape(-1,2)

        # Normalise co-ordinates and multiply by 255 to be the same order of magnitude as RGB data
        xy_coords_1d_array[:,0] = (xy_coords_1d_array[:,0]/xy_coords_1d_array[:,0].max())*255
        xy_coords_1d_array[:,1] = (xy_coords_1d_array[:,1]/xy_coords_1d_array[:,1].max())*255

        # Combine into a single array with 5 columns: R,G,B,X,Y
        pixel_features_5d = np.column_stack((rgb_pixels, xy_coords_1d_array))

        print("... running HDBSCAN to cluster and label image features")

        # Scale features & run HDBSCAN
        X = StandardScaler().fit_transform(pixel_features_5d)
        cluster_threshold = round(len(xy_coords_1d_array) * cluster_sensitivity)
        clusters = hdbscan.HDBSCAN(min_cluster_size=cluster_threshold, min_samples=cluster_threshold)
        labels_1d = clusters.fit_predict(X)
        labels_xy = np.array(labels_1d).reshape(h,w)

        print(f"... image segmented into {len(np.unique(labels_1d))-1} clusters")

        if self.inputs.verbosity > 1:
            plt.figure(figsize=(16, 12))
            plt.hist(labels_1d)
            plt.show()

        # Labels are reverted to the image scale so that the mask can be applied to the original image
        labels_xy_rescale = self._revert_image_scale(labels_1d)

        # TODO: Make the backgound identification process more robust, for example using the histogram in conjunction with the edge process (future release)

        # Get unique values and their frequencies
        values, counts = np.unique(labels_1d, return_counts=True)

        # Find the index of the maximum count and update the background mask assuming it is the most common pixel
        index = np.argmax(counts)
        background_label = values[index]
        background_mask_single_channel = (labels_xy_rescale == background_label)
        self.background_mask = np.stack([background_mask_single_channel] * 3, axis=-1)

        print("... background extraction complete!")

        if self.inputs.verbosity > 0:
            plt.figure(figsize=(16, 12))
            plt.imshow(self.background_mask)
            plt.show()

        if self.inputs.verbosity > 1:
            point_cloud = pv.PolyData(rgb_pixels)
            point_cloud["ClusterID"] = labels_1d
            plotter = pv.Plotter()
            plotter.add_points(point_cloud, point_size=2)
            plotter.show()

        return self

    def _revert_image_scale(self,image: np.ndarray) -> np.ndarray:
        """Rescales image to rgb_raw dimensions using nearest-neighbor interpolation"""

        target_h, target_w, _ = self.rgb_raw.shape
        resized_image = cv2.resize(image.astype(np.uint16), (target_w, target_h), interpolation=cv2.INTER_NEAREST)

        return resized_image

    @staticmethod
    def _adjust_gamma(image: np.ndarray, gamma: float) -> np.ndarray:
        """
        Applies power-law transformation: V_out = V_max * (V_in / V_max)^gamma.
        Args: image (np.ndarray), gamma (float).
        Returns: np.ndarray (float32).
        """

        image_gamma_corrected = image.max() * (image/image.max())**gamma

        return image_gamma_corrected44

    @staticmethod
    def _downsample_with_binning(image: np.ndarray, downsample_factor: int) -> np.ndarray:
        """
        Downsamples a 3D image array via block averaging. Divides the image into (n x n) blocks and replaces each with its mean value.
        Args:
            image: NumPy array of shape (H, W, C) to be downsampled.
            downsample_factor: Integer scale factor for dimension reduction.
        Returns:
            np.ndarray: A new downsampled array of shape (H//n, W//n, C).
        Raises:
            TypeError: If downsample_factor is not an integer.
            ValueError: If downsample_factor is less than 1.
        """

        if not isinstance(downsample_factor, int):
            raise TypeError(f"Downsample factor must be an integer, got {type(downsample_factor).__name__}")

        if downsample_factor < 1:
            raise ValueError(f"Downsample factor must be at least 1: {downsample_factor}")

        # If factor is 1, no processing is required.
        if downsample_factor == 1:
            return image

        # Crop image to be a multiple of the downsample factor in height/width dimensions.
        h, w, c = image.shape
        image_cropped = image[:h - (h % downsample_factor), :w - (w % downsample_factor), :c]

        # Get dimensions of the cropped image.
        h_crop, w_crop, c = image_cropped.shape

        # Reshape the image to move the pixels to be averaged into separate dimensions.
        # Then average across those dimensions to produce the downsampled image.
        image_reshaped = image_cropped.reshape(
            h_crop // downsample_factor, downsample_factor,
            w_crop // downsample_factor, downsample_factor,
            c
        )

        image_downsampled = image_reshaped.mean(axis=(1, 3), dtype=np.float32)

        return image_downsampled

if __name__ == "__main__":
    test_image = StackedImage(data_directory=Path(r""),verbosity=0)
    test_image.correct_white_balance()
    test_image.remove_vignette()
    test_image.boost_dynamic_range(gamma=7)
    test_image.save_image_as_tiff()





